# -*- coding: utf-8 -*-
"""training_emobank_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0MRQZ3m-1pVZBAPdtRoZ7kkWfDMEerF
"""
import itertools
import os
import pickle

import torch
import spacy as sp
import pandas as pd
import numpy as np
from torch import nn
from torch import optim
# from torch.autograd import Variable
import random
import torch.nn.functional as F
# from sklearn import preprocessing
# import sklearn.metrics.pairwise as pairwise
# from datetime import datetime, date, time
# from collections import Counter, defaultdict
# import json
# import matplotlib.pyplot as plt
# from scipy.ndimage.filters import gaussian_filter1d
from sklearn.preprocessing import MinMaxScaler
# import scipy
import warnings

warnings.filterwarnings('ignore')

EMO_VOC_PICKLE_PATH = os.path.join("pickles", "emo_voc.pkl")
DATA_PICKLE_PATH = os.path.join("pickles", "emo_data.pkl")

raw = pd.read_csv('data/EmoBank/corpus/raw.tsv', sep='\t')
wr_emos = pd.read_csv('data/EmoBank/corpus/writer.tsv', sep='\t')

joined = raw.set_index('id').join(wr_emos.set_index('id'))


def normalize(data, target_column_name):
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaler = scaler.fit(data.values.reshape(-1, 1))
    normalized = scaler.transform(data.values.reshape(-1, 1))
    joined[target_column_name] = normalized
    return scaler


arousal_normalizer = normalize(joined["Arousal"], "Arousal_norm")
dominance_normalizer = normalize(joined["Dominance"], "Dominance_norm")
valence_normalizer = normalize(joined["Valence"], "Valence_norm")
joined.head(100)


def get_specific_emotion_values(valence, arousal, dominance):
    anger = np.array([-0.51, 0.59, 0.25])
    disgust = np.array([-0.60, 0.35, 0.11])
    fear = np.array([-0.64, 0.60, -0.43])
    sadness = np.array([-0.63, -0.27, -0.33])
    joy = np.array([0.76, 0.48, 0.35])
    surprise = np.array([0.40, 0.67, -0.13])

    arousal_norm = arousal_normalizer.transform(np.array([arousal]).reshape(-1, 1))[0][0]
    valence_norm = valence_normalizer.transform(np.array([valence]).reshape(-1, 1))[0][0]
    dominance_norm = dominance_normalizer.transform(np.array([dominance]).reshape(-1, 1))[0][0]
    input_arr = np.array([valence_norm, arousal_norm, dominance_norm])

    anger_dist = (input_arr @ anger) / (np.linalg.norm(input_arr) * np.linalg.norm(anger))
    disgust_dist = (input_arr @ disgust) / (np.linalg.norm(input_arr) * np.linalg.norm(disgust))
    fear_dist = (input_arr @ fear) / (np.linalg.norm(input_arr) * np.linalg.norm(fear))
    sadness_dist = (input_arr @ sadness) / (np.linalg.norm(input_arr) * np.linalg.norm(sadness))
    joy_dist = (input_arr @ joy) / (np.linalg.norm(input_arr) * np.linalg.norm(joy))
    surprise_dist = (input_arr @ surprise) / (np.linalg.norm(input_arr) * np.linalg.norm(surprise))
    return anger_dist, disgust_dist, fear_dist, sadness_dist, joy_dist, surprise_dist


get_specific_emotion_values(3.7, 4.2, 3.0)

nlp = sp.load('en_core_web_md')

joined = joined.dropna()


class Vocab:
    def __init__(self, embeddings_file):
        self.vec = []
        self.word_count = 1
        self.ind2word = {0: '<PAD>'}
        self.word2ind = {'<PAD>': 0}
        for line in open(embeddings_file, 'r', encoding="utf8"):
            values = line.split(" ")
            v = []
            for i in range(1, len(values)):
                v.append(float(values[i]))
            self.vec.append(v)
            self.ind2word[self.word_count] = values[0]
            self.word2ind[values[0]] = self.word_count
            self.word_count += 1


if os.path.isfile(EMO_VOC_PICKLE_PATH):
    f_voc = open(EMO_VOC_PICKLE_PATH, "rb")
    lang = pickle.load(f_voc)
else:
    lang = Vocab("data/glove.6B.300d.txt")
    print("Saving vocabulary")
    f_voc = open(EMO_VOC_PICKLE_PATH, "wb")
    pickle.dump(lang, f_voc, protocol=4)
    f_voc.close()
    print("Done!")

print(lang.word2ind['table'])

vocab = lang

MAX_LENGTH = 100
MAX_SD = 0.5


def unseen_text2tensor(text, vocab):
    proc_sentence = nlp(text.lower())
    if len(proc_sentence) > MAX_LENGTH:
        return None
    indexes = []
    for t in proc_sentence:
        if t.text in vocab.word2ind:
            indexes.append(vocab.word2ind[t.text])
    if len(indexes) < 3:
        return None
    input_tensor = torch.LongTensor(indexes).cuda()
    return input_tensor


def row2tensor(row, vocab):
    proc_text = nlp(row['sentence'].lower())
    if len(proc_text) > MAX_LENGTH:
        return None
    indexes = []
    for t in proc_text:
        if t.text in vocab.word2ind:
            indexes.append(vocab.word2ind[t.text])
    if len(indexes) == 0:
        return None
    input_tensor = torch.LongTensor(indexes).cuda()
    target_tensor = torch.FloatTensor([float(row['Valence']), float(row['Arousal']), float(row['Dominance'])]).cuda()
    return input_tensor, target_tensor


def data2tensors(data, vocab):
    instances = []
    for index, row in data.iterrows():
        if row['sd.Valence'] > MAX_SD or row['sd.Arousal'] > MAX_SD or row['sd.Dominance'] > MAX_SD:
            continue
        t = row2tensor(row, vocab)
        instances.append(t)
    return instances


if os.path.isfile(DATA_PICKLE_PATH):
    f_data = open(DATA_PICKLE_PATH, "rb")
    data = pickle.load(f_data)
else:
    data = data2tensors(joined, vocab)
    print("Saving data pickle")
    f_data = open(DATA_PICKLE_PATH, "wb")
    pickle.dump(data, f_data, protocol=4)
    f_data.close()
    print("Done!")

random.choice(data)


class Attn(nn.Module):
    def __init__(self, hidden_size, max_length=MAX_LENGTH):
        super(Attn, self).__init__()

        self.hidden_size = hidden_size
        self.lin = nn.Linear(self.hidden_size * 2, hidden_size * 2)
        self.weight_vec = nn.Parameter(torch.FloatTensor(1, hidden_size * 2))

    def forward(self, outputs):
        batch_size, seq_len = outputs.size(0), outputs.size(1)

        attn_energies = torch.zeros(batch_size, seq_len).cuda()  # B x 1 x S

        for i in range(batch_size):
            for j in range(seq_len):
                attn_energies[i][j] = self.score(outputs[i][j])

        return F.softmax(attn_energies).unsqueeze(0)

    def score(self, output):
        energy = self.lin(output)
        energy = torch.dot(self.weight_vec.view(-1), energy.view(-1))
        return energy


class EmoModel(nn.Module):
    def __init__(self, vocab, hidden_size, output_size, n_layers):
        super(EmoModel, self).__init__()
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers

        # self.embedding = nn.Embedding(input_size, embedding_size)
        self.embedding = nn.Embedding(vocab.word_count, len(vocab.vec[0]), padding_idx=0)
        self.embedding.weight = nn.Parameter(torch.FloatTensor(vocab.vec))
        # self.embedding.weight.requires_grad = False

        self.lstm = nn.LSTM(len(vocab.vec[0]), hidden_size, n_layers, bidirectional=True)
        self.out = nn.Linear(hidden_size * 2, output_size)
        self.attn = Attn(hidden_size)

    def forward(self, input_text, input_lengths):
        batch_size, seq_len = input_text.size(0), input_text.size(1)
        embedded_words = self.embedding(input_text)
        packed = nn.utils.rnn.pack_padded_sequence(embedded_words, input_lengths, batch_first=True)
        last_hidden = self.init_hidden(batch_size)
        rnn_outputs, hidden = self.lstm(packed, last_hidden)
        rnn_outputs, _ = nn.utils.rnn.pad_packed_sequence(rnn_outputs, batch_first=True)
        attn_weights = self.attn(rnn_outputs)
        attn_weights = attn_weights.squeeze(1).view(batch_size, seq_len, 1)
        rnn_outputs = rnn_outputs.squeeze(1)
        attn_weights = attn_weights.expand(batch_size, seq_len, self.hidden_size * 2)
        weigthed_outputs = torch.mul(rnn_outputs, attn_weights)
        output = torch.sum(weigthed_outputs, -2)
        output = self.out(output)
        return output

    def init_hidden(self, bsz):
        return (torch.zeros(self.n_layers * 2, bsz, self.hidden_size).cuda(),
                torch.zeros(self.n_layers * 2, bsz, self.hidden_size).cuda())


BATCH_SIZE = 2
n_layers = 3
hidden_size = 1000

model = EmoModel(lang, hidden_size, 3, n_layers).cuda()

criterion = nn.MSELoss()
learning_rate = 0.0001
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)

# def train_sentence(inputs, targets, criterion, model, optimizer):
#     optimizer.zero_grad()
#     seq_len = len(sentence)
#     loss = 0
#     output = model(sentence)
#     loss = criterion(output, target)
#     loss.backward()
#     optimizer.step()
#     return output, loss.item()
#     # print("gell")


PAD_TOKEN = 0


def zero_padding(l, fillvalue=PAD_TOKEN):
    return list(itertools.zip_longest(*l, fillvalue=fillvalue))


def train_batch(pairs, criterion, model, optimizer):
    optimizer.zero_grad()
    pairs.sort(key=lambda x: len(x[0]), reverse=True)
    inputs = [pair[0] for pair in pairs]
    targets = [pair[1] for pair in pairs]
    lengths = torch.tensor([len(input) for input in inputs])

    inputs = torch.LongTensor(zero_padding(inputs)).t().cuda()
    targets = torch.tensor(zero_padding(targets)).t().cuda()
    outputs = model(inputs, lengths)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    return outputs, loss.item()


def get_batch():
    batch = []
    while len(batch) < BATCH_SIZE:
        pair = random.choice(data)
        if pair is None:
            continue
        batch.append(pair)

    return batch


n_epochs = 2000
print_every = 100
sample_every = 200
loss = 0
for e in range(1, n_epochs + 1):
    batch = get_batch()
    output, iter_loss = train_batch(batch, criterion, model, optimizer)
    loss += iter_loss

    if e % print_every == 0:
        loss = loss / print_every
        print('Epoch %d Current Loss = %.4f' % (e, loss))
        loss = 0
    if e % sample_every == 0:
        # print(tgt)
        print(output)


def serialize():
    torch.save(model.state_dict(),
               "models/emotions.filtered.55Kiter.glove.6B.300d.3L.1000hidden.attention.model")


serialize()


def predict_random():
    pair = random.choice(data)
    inp = pair[0]
    tgt = pair[1]
    output = model(inp)
    return output, tgt


def predict_unseen_text(text, model):
    inp = unseen_text2tensor(text, vocab)
    if inp is not None:
        output = model(inp)
    else:
        output = -1
    return output


predict_unseen_text("What a bad day:(", model)

for i in range(10):
    output, target = predict_random()
    print(output)
    print(target)

predict_unseen_text("This was a very very bad day I am so depressed", model)
